model_name: "models/base"
max_seq_length: 2048
load_in_4bit: true

lora:
  r: 16
  lora_alpha: 16
  lora_dropout: 0.0
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj

training:
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 4
  warmup_steps: 5
  max_steps: 60
  learning_rate: 0.0002
  logging_steps: 1
  optim: adamw_8bit
  bf16: auto
  fp16: auto

output_dir: "models/adapters"
